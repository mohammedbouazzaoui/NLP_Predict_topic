{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KPMG analyse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!python -m spacy download nl_core_news_md\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"nl_core_news_md\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some debugging stuff\n",
    "# \n",
    "if DEBUG:\n",
    "    import inspect \n",
    "    def getname():\n",
    "        #\n",
    "        #Function: getname\n",
    "        #Return: name of function\n",
    "        #\n",
    "        import sys\n",
    "        return sys._getframe(1).f_code.co_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_cleanandlemmatize(txtdoc: str, list_wordstoskip:str = '', onlynouns:bool = True) -> (dict,list):\n",
    "    #\n",
    "    #Function: nlp_cleanandlemmatize\n",
    "    #\n",
    "    #Description: This function will clean a text and lemmatize it , it returns a dictionary off all words and a list of all tokens, only 'NOUNS' are kept.\n",
    "    #Input: txtdoc: str, list_wordstoskip:str = '', onlynouns:bool = True\n",
    "    #Return:(dict,list)\n",
    "    #\n",
    "\n",
    "    if DEBUG : print(\"In function: \",getname(), inspect.signature(globals()[getname()]))\n",
    "   \n",
    "    LANG='nl'\n",
    "    # words to discard\n",
    "    months={'nl':['januari','februari','maart','april','mei','juni','augustus','september','oktober','november','december'],\n",
    "            'fr':['janvier','fevrier','mars','avril','mai','juin','juillet','aout','septembre','octobre','decembre']}\n",
    "    days={'nl':['maandag','dinsdag','woensdag','donderdag','vrijdag','zaterdag','zondag'],\n",
    "            'fr':['lundi','mardi','mercredi','jeudi','vendredi','samedi','dimanche']}\n",
    "    # \n",
    "    nlp.max_length=10000000\n",
    "    nlp_doc=nlp(txtdoc)\n",
    "    list_allwordslemmatized=[]\n",
    "    dict_uniqwords={}\n",
    "    list_tokens=[]\n",
    "    #filter\n",
    "    for token in nlp_doc:\n",
    "        lemma_lower=token.lemma_.lower()\n",
    "        if token in stopwords:\n",
    "            continue\n",
    "        if (token.is_punct or token.is_space or token.is_stop):\n",
    "            continue\n",
    "        if token.text.isdecimal():\n",
    "            continue\n",
    "        if True in [char.isdigit() for char in token.text]:\n",
    "            continue\n",
    "        if token.text[-1] == '.':\n",
    "            continue\n",
    "        if len(token.text) <= 2:\n",
    "            continue\n",
    "        if lemma_lower in months[LANG]:\n",
    "            continue\n",
    "        if lemma_lower in days[LANG]:\n",
    "            continue\n",
    "        if lemma_lower in list_wordstoskip:\n",
    "            continue\n",
    "        #pass only nouns\n",
    "        if token.pos_ != 'NOUN':\n",
    "            continue\n",
    "        #create dict of unique words with count\n",
    "        if lemma_lower not in dict_uniqwords: \n",
    "            dict_uniqwords[lemma_lower]=1\n",
    "            #save tokens for vector comparison\n",
    "            list_tokens.append(token)\n",
    "        else:\n",
    "            dict_uniqwords[lemma_lower]+=1\n",
    "    return dict_uniqwords, list_tokens\n",
    "\n",
    "def token_compare(token_totest,list_tokens,min_score:int = 0.6) -> (bool,float,list):\n",
    "    #\n",
    "    #Function: token_compare\n",
    "    #\n",
    "    #Description: Compare a token against a list of tokens and using a treshold return tokens that have a similarity score higher then this treshold.\n",
    "    #Input: token_totest,list_tokens,min_score:int = 0.6\n",
    "    #Return: (boolean,float,list)\n",
    "    #\n",
    "    \n",
    "    if DEBUG : print(\"In function: \",getname(), inspect.signature(globals()[getname()]))\n",
    "    \n",
    "    #return list of tokens with similarity >= min_score \n",
    "    list_tokens_toreturn=[]\n",
    "    tot_similar_word=0\n",
    "    istax=False\n",
    "    for token in list_tokens:\n",
    "        similar=token_totest.similarity(token)\n",
    "        #only addup the scores >= min_score\n",
    "        if similar >= min_score:\n",
    "            list_tokens_toreturn.append([token_totest,token,similar])\n",
    "            #positive for tax\n",
    "            istax=True\n",
    "            tot_similar_word+=similar\n",
    "    return istax, tot_similar_word, list_tokens_toreturn\n",
    "\n",
    "def createlistofkeywords(numberofdocumenttoscan:int = 50,similar_doc:int = 0,min_score:float = 0.6, list_keywords:list = []) -> (list,list,list):\n",
    "    #\n",
    "    #Function: createlistofkeywords\n",
    "    #\n",
    "    #Description: Discover new keywords using some initial starting keywords, return the expanded list of keywords.\n",
    "    #Input: numberofdocumenttoscan:int = 50,similar_doc:int = 0,min_score:float = 0.6, list_keywords:list = []\n",
    "    #Return: (list,list,list)\n",
    "    #\n",
    "    \n",
    "    if DEBUG : print(\"In function: \",getname(), inspect.signature(globals()[getname()]))\n",
    "     \n",
    "    list_keep_tax_words=[]\n",
    "    list_keep_pointer_taxdocs=[]\n",
    "    list_keep_pointer_alldocs=[]\n",
    "\n",
    "    #take all docs if 0\n",
    "    if numberofdocumenttoscan == 0:\n",
    "        numberofdocumenttoscan=len(df)\n",
    "    \n",
    "    for n in range(numberofdocumenttoscan):\n",
    "\n",
    "        print(\"Documents analyzed: \",numberofdocumenttoscan,n)\n",
    "\n",
    "        txtdoc = df['cleantextnl'].values[n]\n",
    "        list_wordstoskip=['blabla','blablabla'] #add here words to discard\n",
    "        dict_uniqwords,list_tokens=nlp_cleanandlemmatize(txtdoc,list_wordstoskip) #get tokens\n",
    "\n",
    "        #tokenize and check\n",
    "        tot_similar_doc=0     #keep score for onlytax docs (istax=True)\n",
    "        tot_similar_doc_all=0 #keep score for all docs\n",
    "        istax_doc=False\n",
    "        \n",
    "        for word in list_keywords:\n",
    "            token_word = nlp(word)\n",
    "            istax, tot_similar_word, list_res_tokens=token_compare(token_word,list_tokens,min_score)\n",
    "            tot_similar_doc_all+=tot_similar_word\n",
    "\n",
    "            if istax:\n",
    "                for taxtoken in list_res_tokens:\n",
    "                    if taxtoken[1].lemma_.lower() not in list_keep_tax_words:\n",
    "                        list_keep_tax_words.append(taxtoken[1].lemma_.lower())\n",
    "                tot_similar_doc+=tot_similar_word\n",
    "            istax_doc |= istax\n",
    "\n",
    "        #store score for all\n",
    "        list_keep_pointer_alldocs.append([n,tot_similar_doc_all])\n",
    "\n",
    "        #store score for taxdocs\n",
    "        if istax_doc and (tot_similar_doc >= similar_doc):\n",
    "            list_keep_pointer_taxdocs.append([n,tot_similar_doc])\n",
    "\n",
    "    return list_keep_pointer_taxdocs, list_keep_tax_words, list_keep_pointer_alldocs\n",
    "\n",
    "def create_pickle_keywords_and_docscores(list_keywords:list = ['belasting'], file_keywords:str = \"\", file_docscores:str = \"\")-> (list,list):\n",
    "    #\n",
    "    #Function: create_pickle_keywords_and_docscores\n",
    "    #\n",
    "    #Description: Discover and save a keyword list and a document score list. \n",
    "    #Input: list_keywords:list = ['belasting'], file_keywords:str = \"\", file_docscores:str = \"\"\n",
    "    #Return: (list,list)\n",
    "    #\n",
    "    \n",
    "    if DEBUG : print(\"In function: \",getname(), inspect.signature(globals()[getname()]))\n",
    "     \n",
    "    #settings : [numberofdocumenttoscan (0 for all), min similarity score for doc to get into the taxlist, min similarity score for keywords]):\n",
    "    settings=[  [5,5,0.97],    #step1\n",
    "                [10,10,0.95],  #step2\n",
    "                [15,20,0.90],  #step3\n",
    "                [0,40,0.87]    #step4 All documents\n",
    "                ]\n",
    "    \n",
    "    numberofsteps=len(settings)\n",
    "    for step in range(numberofsteps):\n",
    "        list_keep_pointer_taxdocs , list_keep_tax_words, list_keep_pointer_alldocs = createlistofkeywords(settings[step][0],settings[step][1],settings[step][2],list_keywords)\n",
    "        list_keywords+=list_keep_tax_words\n",
    "        #no duplicates\n",
    "        list_keywords = list(set(list_keywords))\n",
    "        list_docscores=list_keep_pointer_alldocs\n",
    "        \n",
    "        # no file no pickle\n",
    "        if file_keywords != \"\" :\n",
    "            df_keywords=pd.DataFrame(list_keywords,columns=['keywords'])\n",
    "            df_keywords.to_pickle(file_keywords)  \n",
    "            \n",
    "        if file_docscores != \"\"  :\n",
    "            df_docscores=pd.DataFrame(list_keep_pointer_alldocs,columns=['docpointer','docscores']) \n",
    "            df_docscores.to_pickle(file_docscores)  \n",
    "\n",
    "    return list_keywords, list_docscores\n",
    "\n",
    "def score_text(txt:str,language:str = 'nl', min_score:float = 0.3, file_keywords:str = \"../data/tax_keywords_nl.pkl\") -> float :\n",
    "    #\n",
    "    #Function: score_text\n",
    "    #\n",
    "    #Description: Using list of keywords score of how related a document is to these keywords by using similarity.\n",
    "    #Input: txt:str,language:str = 'nl', min_score:float = 0.3, file_keywords:str = \"../data/tax_keywords_nl.pkl\"\n",
    "    #Return: float\n",
    "    #\n",
    "    \n",
    "    if DEBUG : print(\"In function: \",getname(), inspect.signature(globals()[getname()]))\n",
    "\n",
    "    if language == 'nl':\n",
    "        df_keywords = pd.read_pickle(file_keywords) \n",
    "        list_keywords=list(df_keywords['keywords'])\n",
    "        \n",
    "        #clean txt / get tokens\n",
    "        dict_uniqwords,list_tokens=nlp_cleanandlemmatize(txt,[]) \n",
    "        \n",
    "        docscore=0  \n",
    "        for word in list_keywords:\n",
    "            token_word = nlp(word)\n",
    "            for token in list_tokens:\n",
    "                similar=token_word.similarity(token)\n",
    "                #similarity > min_score to be taken into account\n",
    "                if similar >= min_score:\n",
    "                    docscore+=similar\n",
    "    else:\n",
    "        print(\"Language selection not supported for now!\")\n",
    "\n",
    "    return docscore\n",
    "\n",
    "def create_initial_keywordlist(language:str ='nl') -> None:\n",
    "    #\n",
    "    #Function: create_initial_keywordlist\n",
    "    #\n",
    "    #Description: Create the initial keyword list and save it.\n",
    "    #Input: language:str ='nl'\n",
    "    #Return: None\n",
    "    #\n",
    "    \n",
    "    # create keyword list picklefile and docscores picklefile\n",
    "    #read all docs\n",
    "    global df\n",
    "    df = pd.read_pickle(\"../data/Staatsblad_nl_fr.pkl\") \n",
    "    if language == 'nl':\n",
    "        #start search for nl keywords\n",
    "        keywords, docscores = create_pickle_keywords_and_docscores(['belasting','tax','fisc'], \"../data/tax_keywords_nl.pkl\", \"../data/tax_docscores_nl.pkl\")\n",
    "    else:\n",
    "        print(\"Language selection not supported for now!\")\n",
    "\n",
    "\n",
    "#unsupervised keyword search\n",
    "def get_keywordsunsupervised(txt:str, sim:float = 0.90) -> dict:\n",
    "    #\n",
    "    #Function: get_keywordsunsupervised\n",
    "    #\n",
    "    #Description: Discover keywords that have the most similarity towards the textcontent unsupervised keyword search.\n",
    "    #Input: txt:str, sim:float = 0.90\n",
    "    #Return: dict\n",
    "    #\n",
    "    \n",
    "    doc = nlp(txt)\n",
    "\n",
    "    chnk=[]\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chnk.append(chunk)\n",
    "\n",
    "    sim_low=sim\n",
    "    txt_keywords=''\n",
    "    list_keywordswithscores=[]\n",
    "    for c in chnk:\n",
    "        word_simil=0\n",
    "        for t in chnk:\n",
    "            simil=c.similarity(t)\n",
    "            if (simil >= sim_low and simil < 1): \n",
    "                word_simil+=simil\n",
    "                \n",
    "        if word_simil > 1:\n",
    "            list_keywordswithscores.append([c,word_simil])\n",
    "            txt_keywords=txt_keywords + c.lemma_ +' '\n",
    "            \n",
    "    dict_txt,list_tokens=nlp_cleanandlemmatize(txtdoc=txt_keywords,list_wordstoskip='',onlynouns=True)\n",
    "    return dict_txt\n",
    "\n",
    "def score_topic_list(txt:str,language:str = 'nl', min_score:float = 0.3, list_topics:list = ['belasting','tax']) -> float :\n",
    "    #\n",
    "    #Function: score_topic_list\n",
    "    #\n",
    "    #Description: Calculate a score for a list of keywords towards a text.\n",
    "    #Input: txt:str,language:str = 'nl', min_score:float = 0.3, list_topics:list = ['belasting','tax']\n",
    "    #Return: float\n",
    "    #\n",
    "    \n",
    "    if DEBUG : print(\"In function: \",getname(), inspect.signature(globals()[getname()]))\n",
    "\n",
    "    if language == 'nl':\n",
    "       \n",
    "        #clean txt / get tokens\n",
    "        dict_uniqwords,list_tokens=nlp_cleanandlemmatize(txt,[]) \n",
    "\n",
    "        topic_score=[]\n",
    "       \n",
    "        for word in list_topics:\n",
    "            docscore=0\n",
    "            token_word = nlp(word)\n",
    "            for token in list_tokens:\n",
    "                similar=token_word.similarity(token)\n",
    "                if similar >= min_score:\n",
    "                    docscore+=similar\n",
    "            topic_score.append(docscore)\n",
    "    else:\n",
    "        topic_score=[]\n",
    "        print(\"Language selection not supported for now!\")\n",
    "\n",
    "    return topic_score   \n",
    "\n",
    "\n",
    "def score_text_byvector(txt:str,language:str = 'nl', min_score:float = 0.3, file_keywords:str = \"../data/tax_keywords_nl.pkl\") -> float :\n",
    "    #\n",
    "    #Function: score_text_byvector\n",
    "    #\n",
    "    #Description: Score a document using similarity of a document vector and the kyword list vector.\n",
    "    #Input: txt:str,language:str = 'nl', min_score:float = 0.3, file_keywords:str = \"../data/tax_keywords_nl.pkl\"\n",
    "    #Return: float\n",
    "    #\n",
    "    \n",
    "    if language != 'nl':\n",
    "        print(\"Language selection not supported for now!\")\n",
    "        return -999\n",
    "\n",
    "    df_keywords = pd.read_pickle(file_keywords) \n",
    "    list_keywords=list(df_keywords['keywords'])\n",
    "\n",
    "    txt_keywords=''\n",
    "    for t in list_keywords: txt_keywords += ' ' + t\n",
    "    token_txt = nlp(txt_keywords)\n",
    "\n",
    "    #clean txt  \n",
    "    txt_doc=''\n",
    "    dict_uniqwords,list_tokens=nlp_cleanandlemmatize(txt,[]) \n",
    "    for i in list_tokens: txt_doc += ' ' + (i.lemma_).lower()\n",
    "    token_doc = nlp(txt_doc)\n",
    "\n",
    "    docscore=token_doc.similarity(token_txt)   \n",
    "\n",
    "    return docscore\n",
    "\n",
    "\n",
    "def get_topic_byvector(txt:str, language:str = 'nl') -> float:\n",
    "    #\n",
    "    #Function: get_topic_byvector\n",
    "    #\n",
    "    #Description: This function predicts the main topic in a document by using a list of topic keywords.\n",
    "    #Input: txt:str, language:str = 'nl'\n",
    "    #Return: float\n",
    "    #\n",
    "    \n",
    "    if language != 'nl':\n",
    "        print(\"Language selection not supported for now!\")\n",
    "        return -999\n",
    "    list_topic_keywords = [ ['inkomstenbelasting'],\n",
    "                            ['personenbelasting'],\n",
    "                            ['vennootschapsbelasting'],\n",
    "                            ['rechtspersonenbelasting'],\n",
    "                            ['belasting van niet-inwoners'],\n",
    "                            ['belasting op de toegevoegde waarde'],\n",
    "                            ['internationale belastingrecht'],\n",
    "                            ['registratierechten'],\n",
    "                            ['successierechten'],\n",
    "                            ['douanerechten'],\n",
    "                            ['verkeersbelasting'],\n",
    "                            ['loonbelasting'],\n",
    "                            ['dividendbelasting'],\n",
    "                            ['erfbelasting'],\n",
    "                            ['schenkbelasting'],\n",
    "                            ['kansspelbelasting'],\n",
    "                            ['gokbelasting'],\n",
    "                            ['vermogensrendementsheffing']\n",
    "                            ]\n",
    "\n",
    "\n",
    "    #clean text  \n",
    "    txt_doc=''\n",
    "    dict_uniqwords,list_tokens=nlp_cleanandlemmatize(txt,[]) \n",
    "    for i in list_tokens: txt_doc += ' ' + (i.lemma_).lower()\n",
    "\n",
    "    #\n",
    "    topicscore=[]\n",
    "    for list_keywords in list_topic_keywords:\n",
    "        txt_keywords=''\n",
    "        for t in list_keywords: txt_keywords += ' ' + t\n",
    "        token_txt = nlp(txt_keywords)\n",
    "        token_doc = nlp(txt_doc)\n",
    "\n",
    "        topicscore.append([token_doc.similarity(token_txt),list_keywords])   \n",
    "        topicscore.sort(reverse=True)\n",
    "        score = topicscore[0]\n",
    "    return topicscore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will create the inital keyword list (takes some time to run +-7min on my old laptop) and needs to be run only once!\n",
    "#df = pd.read_pickle(\"../data/Staatsblad_nl_fr.pkl\") \n",
    "\n",
    "#create_initial_keywordlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bmadmin\\AppData\\Local\\Temp\\ipykernel_1312\\63596739.py:366: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  topicscore.append([token_doc.similarity(token_txt),list_keywords])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc. nr: 0  --->>  [[0.7487207824085366, ['vennootschapsbelasting']], [0.7487207824085366, ['rechtspersonenbelasting']], [0.7487207824085366, ['dividendbelasting']], [0.7395439827552766, ['internationale belastingrecht']], [0.7247386939478287, ['vermogensrendementsheffing']], [0.7247386939478287, ['schenkbelasting']], [0.7247386939478287, ['personenbelasting']], [0.7247386939478287, ['loonbelasting']], [0.7247386939478287, ['kansspelbelasting']], [0.7247386939478287, ['inkomstenbelasting']], [0.7247386939478287, ['erfbelasting']], [0.7077180633342771, ['successierechten']], [0.6768987499247198, ['douanerechten']], [0.6589777626921278, ['registratierechten']], [0.6404529739562098, ['verkeersbelasting']], [0.6229665826607022, ['belasting van niet-inwoners']], [0.5645308553529333, ['belasting op de toegevoegde waarde']], [0.0, ['gokbelasting']]]\n",
      "------------------------------------------------------\n",
      "Doc. nr: 1  --->>  [[0.6901101525735599, ['successierechten']], [0.686147257226492, ['vennootschapsbelasting']], [0.686147257226492, ['rechtspersonenbelasting']], [0.686147257226492, ['dividendbelasting']], [0.6767867970527526, ['registratierechten']], [0.6754726173805241, ['vermogensrendementsheffing']], [0.6754726173805241, ['schenkbelasting']], [0.6754726173805241, ['personenbelasting']], [0.6754726173805241, ['loonbelasting']], [0.6754726173805241, ['kansspelbelasting']], [0.6754726173805241, ['inkomstenbelasting']], [0.6754726173805241, ['erfbelasting']], [0.6690303493554342, ['douanerechten']], [0.6659515960892417, ['internationale belastingrecht']], [0.6261573410478103, ['verkeersbelasting']], [0.538156591536086, ['belasting van niet-inwoners']], [0.5111875660119544, ['belasting op de toegevoegde waarde']], [0.0, ['gokbelasting']]]\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Example usage for get_topic_byvector\n",
    "###############################################\n",
    "df = pd.read_pickle(\"../data/Staatsblad.pkl\") \n",
    "\n",
    "for loop in range(2):\n",
    "    txt=df['cleantextnl'][loop]\n",
    "    topicscore=get_topic_byvector(txt)\n",
    "\n",
    "    #topic.sort(reverse=True)\n",
    "    print(\"Doc. nr:\",loop,\" --->> \",topicscore)\n",
    "    print('------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bmadmin\\AppData\\Local\\Temp\\ipykernel_1312\\63596739.py:194: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  similar=token_word.similarity(token)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for d: 277.2337829448818 score for d2: 0\n"
     ]
    }
   ],
   "source": [
    "#Get score for one txt string\n",
    "#############################\n",
    "#This will use the pickled keyword list created by the create_initial_keywordlist() function\n",
    "txt='de belastingen zijn er weer\\n Deze tax keer meer belastingen en meer tax te betalen!\\n meer en meer belastingen tax is nodig en fisc'\n",
    "txt2='this text does not contain any ... related words\\n '\n",
    "d = score_text(txt)\n",
    "d2 = score_text(txt2)\n",
    "print(\"score for d:\",d,\"score for d2:\",d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC DOC 0:{'houdbaarheidsdatum': 2, 'bewoner': 1, 'applicatie': 1, 'medewerker': 1, 'koffiekoek': 1, 'reactie': 1, 'overschot': 1}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bmadmin\\AppData\\Local\\Temp\\ipykernel_1312\\63596739.py:245: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  simil=c.similarity(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'onder_wijswetgeving': 1, 'onderwijs': 9, 'artikel': 3, 'regering': 12, 'voorwaarde': 3, 'organisatie': 8, 'onderwijsactiviteit': 2, 'studie': 7, 'type': 7, 'inspecteur': 1, 'akkoordbevinding': 1, 'minister': 2, 'lid': 17, 'wet': 1, 'opschorting': 1, 'noodzakelijkheid': 3, 'vast_stellen': 1, 'maatregel': 3, 'aandacht': 1, 'instelling': 5, 'omzendbrief': 2, 'nummer': 1, 'aanneming': 1, 'voordracht': 1, 'gezondheidscrisis': 1, 'regeling': 2, 'student': 7, 'onderwijseenheden': 1, 'einddatum': 1, 'eenheid': 1, 'lesti_jd': 1, 'lestijd': 1, 'punt': 1, 'ond_erwijswetgeving': 1, 'herinsc_hrijving': 1, 'onderwijseenhed': 1, 'opsluiting': 1, 'zitting': 3, 'onderwijsinstelling': 2, 'paragraaf': 1, 'evaluatie': 8, 'leerresultaten': 3, 'directie': 2, 'toelating': 2, 'lee_rresultat': 2, 'nazicht': 2, 'onderwijseenheid': 2, 'regel': 3, 'plan': 1, 'evaluatiedatum': 1, 'aard': 1, 'kenmerk': 1, 'materieel': 1, 'omstandigheid': 1, 'mededeling': 1, 'beoordeling': 1, 'eindevaluatie': 2, 'begind_atum': 3, 'startdatum': 1, 'certificatie': 2, 'noemen': 1, 'uitvoering': 1}\n",
      "-----------------------\n",
      "{'steun': 1, 'ziekenhuis': 12, 'artikel': 5, 'financiering': 1, 'dienst': 1, 'regering': 7, 'inspecteur': 1, 'akkoordbevinding': 1, 'minister': 6, 'beleid_lijnen': 1, 'verspreiding': 1, 'bevoegdheid': 2, 'gevolg': 1, 'decreet': 1, 'uitoefening': 1, 'oprichting': 2, 'toename': 1, 'afdeling': 2, 'organisatie': 1, 'opname': 1, 'mobilisatie': 1, 'noodzaak': 1, 'bestrijding': 1, 'voordracht': 1, 'ziekenhuizen': 1, 'voorwaarde': 2, 'toekenning': 2, 'opdracht': 1, 'uitgave': 1, 'inrichting': 1, 'aanpassing': 1, 'opvang': 2, 'ziekenhuisopname': 2, 'aankoop': 1, 'uit_rusting': 1, 'condi_tionering': 1, 'versterking': 1, 'apparatuur': 1, 'analyselaboratoria': 1, 'apotheek': 1, 'subsidie': 3, 'overnachting': 1, 'administratie': 1, 'hoofd': 1, 'bewijsstukk': 1, 'regel': 1, 'uitvoering': 1}\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "#example for unsupervised function\n",
    "#########################################################\n",
    "# test with a file containing NL text\n",
    "#\n",
    "with open('../data/text.txt', encoding=\"utf8\") as file:\n",
    "    txt = file.read()\n",
    "#print(txt)\n",
    "dict_text=get_keywordsunsupervised(txt,0.9)\n",
    "print(\"TOPIC DOC {}:{}\\n\".format(0,dict_text))\n",
    "\n",
    "\n",
    "#example for unsupervised\n",
    "# test with a pandas NL text\n",
    "#\n",
    "df = pd.read_pickle(\"../data/Staatsblad_nl_fr.pkl\") \n",
    "#testing with txt at pos 100 en 101\n",
    "for i in range(2):\n",
    "    dict_text=get_keywordsunsupervised(df['cleantextnl'][i+100],0.90)\n",
    "    print(dict_text)\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS THE END FOR NOW ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f76d795eeeb227e2159e85f1e474be6be927cb377d2f5f811780d197b385e423"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
